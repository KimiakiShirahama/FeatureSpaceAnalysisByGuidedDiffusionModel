# FeatureSpaceAnalysisByGuidedDiffusionModel
This is the official implementation of the decoder introduced in the paper "Feature Space Analysis by Guided Diffusion Model" 

One of the key issues in Deep Neural Networks (DNNs) is the black-box nature of their internal feature extraction process. Targeting vision-related domains, this paper focuses on analysing the feature space of a DNN by proposing a *decoder* that can generate images whose features are guaranteed to closely match a user-specified feature. Owing to this guarantee that is missed in past studies, our decoder allows us to evidence which of various attributes in an image are encoded into a feature by the DNN, by generating images whose features are in proximity to that feature. Our decoder is implemented as a *guided diffusion model* that guides the reverse image generation of a pre-trained diffusion model to minimise the Euclidean distance between the feature of a clean image estimated at each step and the user-specified feature. One practical advantage of our decoder is that it can analyse feature spaces of different DNNs with no additional training and run on a single COTS GPU. The experimental results targeting CLIP's image encoder, ResNet-50 and vision transformer demonstrate that images generated by our decoder have features remarkably similar to the user-specified ones and reveal valuable insights into these DNNs' feature spaces.

<img width="800" src="https://github.com/KimiakiShirahama/FeatureSpaceAnalysisByGuidedDiffusionModel/blob/main/data/overview.png">

## Instllation
Our decoder has been implemented by modifying a [**universal guided diffusion model** developed by A. Bansal *et al.*](https://github.com/arpitbansal297/Universal-Guided-Diffusion) and runs in the same environment as it. This environment can be constructed by the following commands:   
```
conda env create -f environment.yaml
conda activate ldm
conda install pytorch torchvision cudatoolkit=11.3 -c pytorch
pip install GPUtil
pip install blobfile
pip install facenet-pytorch # This is necessary for pytorch's compatibility with CUDA
pip install --upgrade transformers huggingface_hub requests
```

To ease the reproduction of our decoder's results, we release Dockerfile to create the same environment as ours in which our decoder is implemented and tested on Ubuntu 22.04 and CUDA 11.7.1. The other libraries we used can also be seen in this Dockerfile. Furthermore, our anaconda environment was created by [this installer](https://repo.anaconda.com/archive/Anaconda3-2024.06-1-Linux-x86_64.sh).

The stable diffusion (`sd-v1-4.ckpt`) that is used as a background diffusion model of our decoder can be found [here](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original) and should be placed in `models/ldm/`. 

## Basic variables to run our decoder

- `--optim_forward_guidance`: Flag to use the guidance to generate an image whose feature minimises its squared Euclidean distance to a user-specified one
- `--optim_forward_guidance_wt`: Weight for the graident of the loss fuction ($w_g$ in the paper)
- `--optim_num_steps`: Number of self-recurrence iterations ($K$ in the paper, multiple numbers can be specified)
- `--optim_early_emp_end`: Index of the last step for which the early step emphasis is used ($T - T'$ in the paper)
- `--optim_grad_clip_threshold`: Threshold for gradient clipping ($\nabla_{thres}$ in the paper)
- `--optim_folder`: Name of the output directory where the generated image as well as images at intermediate steps and the log file are stored. The finally generated image and the best image are saved into `out_img.png` and `out_img_best_t_<step #>_rec_<self rec. #>_bk_-1_sim_-<squared Euclidean distance>.png`, respectively. 
- `--prompt`: Used to specify an image (png or jpg) or a text prompt from which the user-specified feature is extrated  

Please check `scripts/clip_guided.py`, `scripts/resnet50_guided.py` or `scripts/vith14_guided.py` for more details about variables.

In addition, when you want to analyse the feature space of another feature extractor, the thing you need to do is only to copy one of these script files and change the class handling the feature extraction process (i.e., `Clip` in `scripts/clip_guided.py`, `Resnet50` in `scripts/resnet50_guided.py` and `VitH14` in `scripts/vith14_guided.py`). 

## Image generation by targeting CLIP's image encoder with ResNet-50 backbone

Using [CLIP's image encoder with ResNet-50 backbone](https://github.com/openai/CLIP) as the target feature extractor, the following command is used to generate an image whose feature closely matches the feature of the actual image, which is located at the top-left of Fig. 3 in the paper:
```
python scripts/clip_guided.py --optim_forward_guidance --optim_forward_guidance_wt 4 --optim_num_steps 1000 8 --optim_early_emp_end 995 --optim_grad_clip_threshold 3.0 --optim_folder data/bansal/og_img_0.png.generated_check --optim_print --prompt data/bansal/og_img_0.png
```

The following command is used to generate an image whose feature closely matches the feature of the text prompt "A black motorbike parked on the dry ground", corresponding to the top-left caption in Fig. 8 of the paper:
```
python scripts/clip_guided2.py --optim_forward_guidance --optim_forward_guidance_wt 4 --optim_num_steps 1000 8 --optim_early_emp_end 995 --optim_grad_clip_threshold 3.0 --optim_folder data/mscoco_data/txt_1.generated --optim_print --prompt "a black motorbike parked on the dry ground" 
```

## Image generation by targeting ResNet-50

Using ResNet-50, especially [ResNet-50 configured with ResNet50_Weights.IMAGENET1K_V1](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights), as the target feature extractor, the following command is used to generate an image whose feature closely matches the feature of the actual image, which is located at the top-left of Fig. 5 in the paper:
```
python scripts/resnet50_guided.py --optim_forward_guidance --optim_forward_guidance_wt 4 --optim_num_steps 1000 8 --optim_early_emp_end 995 --optim_grad_clip_threshold 3.0 --optim_folder data/bansal/og_img_0.png.generated_check_res --optim_print --prompt data/bansal/og_img_0.png
```

## Image generation by targeting Vision Transformer

Using vision transformer, especially [ViT-H/14 configured with ViT_H_14_ Weights.IMAGENET1K_SWAG_E2E_V1](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights), as the target feature extractor, the following command is used to generate an image whose feature closely matches the feature of the actual image, which is located at the top-left of Fig. 7 in the paper:
```
python scripts/vith14_guided.py --optim_forward_guidance --optim_forward_guidance_wt 4 --optim_num_steps 1000 8 --optim_early_emp_end 995 --optim_grad_clip_threshold 2.0 --optim_folder data/bansal/og_img_0.png.generated_check_res --optim_print --prompt data/bansal/og_img_0.png
```

## High-resolution figures
We had to compress the following figures containing many images due to the file size limitation during the paper preparation. Please click the links below to see the high-resolution versions: 
- [Figure 3](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/Ec8ZKwGRp8lDgrXJx8uRQa4BDYVSYaxCkAGU1OjiiVn81w?e=rXmJK2)
- [Figure 4](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/ERtabsfWrURPqZvrOMdnBb0B4eqCs9IdN9SmmODKLhovFw?e=Lbr67P)
- [Figure 5](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/EeVv0oVRlQpKuVrSrfbJSg8B40TkDFAqWsqZw5dDMkjtqA?e=w676TZ)
- [Figure 6](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/EdemQyh1zxtIgzj2jQ4UlMABWQj67FPOMmf8YTk5_Xg69w?e=NE0Xey)
- [Figure 7](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/EVwqCxbTlIFAvuCxaTjG9QwBAsbjKkqhd7i9HbMwx4fTiw?e=j2YPnj)
- [Figure 8](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/EYQFEBjkf_hDmOCKmWYkc1sBGbdqOHLH4b3SYMXj_1tO1w?e=RGOvPa)
- [Figure 9](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/EYvKeYjCbHBKhtTiUodXJ6QBFYtUzIzsLHFwf66OKCcguA?e=ANYCNB)
- [Figure 11](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/EdyUQ1zCqVJJq0w4_96EhPQBSWzJ0AyQCVy_k3_Onm1YEg?e=f1RBCQ)
- [Figure 12](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/ETODzGQpqv1Ku-m7v8hhFmUBXKbqO2ClsXq566ql1v96kQ?e=dSIUwf)
- [Figure 13](https://doshishaacjp-my.sharepoint.com/:b:/g/personal/kshiraha_mail_doshisha_ac_jp/EURmEACQRlREp9uJpZkKvL0B6ple-gs-D3UpkXbqOwsHRw)

## Reference
If you found our decoder useful, please cite the following paper:

```
@article{shirahama2025fsa,
    title={Feature Space Analysis by Guided Diffusion Model},
    author={Shirahama, Kimiaki and Yanobu, Miki and Yamashita, Kaduki and Ohsaki, Miho},
    journal={arXiv e-prints arXiv:2509.07936},
    year={2025}
}
```

## License

[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)
